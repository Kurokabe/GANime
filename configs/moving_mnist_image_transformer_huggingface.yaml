model:
  # transformer_config:
  #   vocab_size: 512
  #   block_size: 512
  #   n_layer: 24
  #   n_head: 16
  #   n_embedding: 1024
  #   embedding_percentage_drop: 0.1
  #   attention_percentage_drop: 0.1
  first_stage_config:
    checkpoint_path: ../../../checkpoints/mnist_image/checkpoint
    vqvae_config:
      beta: 0.25
      num_embeddings: 128
      embedding_dim: 128
    autoencoder_config:
      z_channels: 128
      channels: 128
      channels_multiplier: 
      - 1
      - 1 
      - 2
      - 2
      num_res_blocks: 1
      attention_resolution: 
      - 16
      resolution: 128
      dropout: 0.0
    discriminator_config:
      num_layers: 3
      filters: 64
      
    loss_config:
      discriminator:
        loss: "hinge"
        factor: 1.0
        iter_start: 10000
        weight: 0.3
      vqvae:
        codebook_weight: 1.0


